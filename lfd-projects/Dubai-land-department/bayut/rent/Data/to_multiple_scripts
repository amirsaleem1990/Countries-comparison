script = """
import pandas as pd
import requests
import pickle
from bs4 import BeautifulSoup
errors = []
import pickle

all_records_list = []

with open("../all_adds_link.pkl", "rb") as file:
	all_adds_links = pickle.load(file)
for e, url in enumerate(all_adds_linksabcedf):
    print(e, end=", ")
    try:
        soup = BeautifulSoup(requests.get("https://www.bayut.com/for-sale/property/uae" + url).text, "lxml")
        a = [i.text for i in soup.find("ul", {"class" : "_033281ab"}).select("span", {"class" : "_3af7fa95"})]
        b = []
        for i in a:
            if not i.isnumeric():
                if not i in b:
                    b.append(i)
            else:
                b.append(i)
        one_record_dict = dict(zip(b[::2], b[1:][::2]))
        all_records_list.append(one_record_dict)
    except:
        errors.append(url)
        pass

df = pd.DataFrame(all_records_list)

with open("dfabcedf.pkl", "wb") as file:
    pickle.dump(df, file)
with open("errorsabcedf.pkl", "wb") as file:
    pickle.dump(errors, file)
with open("all_records_listabcedf.pkl", "wb") as file:
    pickle.dump(all_records_list, file)
"""

for e, i in enumerate(["[: 2000]", 
		"[2001 : 4000]", 
		"[4001 : 6000]", 
		"[6001 : 8000]", 
		"[8001 : 10000]", 
		"[10001 : 12000]", 
		"[12001 : 14000]", 
		"[14001 : 16000]", 
		"[16001 : 18000]", 
		"[18001 : 20000]", 
		"[20001 : 22000]", 
		"[22001 : 24000]", 
		"[24001 : 26000]", 
		"[26001 : 28000]", 
		"[28001 : 30000]", 
		"[30001 : 32000]", 
		"[32001 : 34000]", 
		"[34001 : 36000]", 
		"[36001 : 38000]", 
		"[38001 : 40000]",
		"[40001 : 42000]",
		"[42001 : 44000]",
		"[44001 : ]"]):
	a = script.replace("abcedf", i)
	with open(str(e)+".py", "w") as file:
		file.write(a)
